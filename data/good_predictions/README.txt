This folder contains the "good" qual and "probe" predictions we got for
single-algorithm and multi-algorithm approaches. The files' names specify
the kind of algorithm, the dataset that we trained on, and our percentage
above water. For instance, SVDPP_PROBE_4.5.dta would mean that file
contains ratings generated by SVD++ on the "probe" dataset that brought us
4.5% above water.  Combined/blended algorithms can be denoted with extra
underscores, or they can have names like BLEND1_QUAL_6.0.dta. The
correspondence between blend names and their contents should be included
below.



Further details on predictions in this folder:

TIMESVDPP_QUAL_7.387: This was generated on May 16 at 12:44 am. Time-SVD++
was carried out on the entire dataset with 100 factors, 60 iterations, and
a learning rate decay of 0.92. Also, all of the regularization factors in
TIMESVDPP_QUAL_7.749 were decreased by a factor of 1.02, to try and get
more overfitting.


TIMESVDPP_QUAL_7.046: This was generated on May 15 at 7:46 pm. Time-SVD++
was carried out on the entire dataset with 200 factors, 80 iterations, and
a learning rate decay of 0.945. All other parameters were the same as those
in TIMESVDPP_QUAL_7.749.


SVD_QUAL_6.372: This was generated on May 15 around 7 pm. SVD was run on
the entire dataset with 2000 factors, 80 iterations, and a learning rate
decay of 0.945. The remaining parameters used were the same as those in 
SVD_QUAL_6.284.


SVDPP_QUAL_6.509: This was generated on May 15 at 3:42 pm. SVD++ was
carried out on the entire dataset with 60 factors and 40 iterations. All
other parameters were the same as those in SVDPP_QUAL_6.833.


RBM_QUAL_4.522: This was generated on May 15 around 1 am. RBM was run on
the entire dataset with 400 factors and 60 iterations. The parameters used
were the same as those in RBM_QUAL_4.952.


RBM_QUAL_4.952: This was generated on May 15 around 7 am. RBM was run on
the entire dataset with 200 factors and 60 iterations. The remaining
parameters used were:
    * alpha = 0.001
    * beta = 0.002
    * dec = 0.001


SVD_QUAL_6.284: This was generated on May 14 at 11:57 pm. An overfitted
(regular) SVD run was carried out with 1000 factors, 80 epochs, and a
learning rate decay of 0.95. The following LRs/RFs were used:
    * Regularization factors:
        * SVD_LAM_B_I = 0.007
        * SVD_LAM_B_U = 0.007
        * SVD_LAM_Q_I = 0.014
        * SVD_LAM_P_U = 0.014
    * Learning rates:
        * SVD_GAMMA_B_I = 0.007
        * SVD_GAMMA_B_U = 0.007
        * SVD_GAMMA_Q_I = 0.007
        * SVD_GAMMA_P_U = 0.007


RBM_QUAL_4.237: This was generated on May 14. RBM was run on base with 200
factors and 36 iterations. The following parameters were used:
    * rbm_alpha = 0.001
    * rbm_beta = 0.002
    * dec = 0.0001


KNN_ON_TIMESVDPP_QUAL_7.284: This was generated on May 14 at 1:02 pm. kNN
with MIN_COMMON = 24 and MAX_WEIGHT = 400 was carried out on the residuals
of a Time-SVD++ run with 60 factors, 40 iterations, and 30 time bins (the
remaining parameters for the Time-SVD++ were the same as those in
TIMESVDPP_QUAL_7.749).


SVDPP_QUAL_7.069: This was generated on May 14 at 10:57 am. An overfitted
SVD++ run was carried out with 2000 factors, 80 iterations, and a learning
rate decay of 0.935. All other parameters (LRs/RFs) were unchanged relative
to SVDPP_QUAL_6.833.


TIMESVDPP_QUAL_7.216: This was generated on May 14 at 9:25 am. An
overfitted Time-SVD++ was carried out with 300 factors, 80 iterations, and
a learning rate decay of 0.945. All other parameters (LRs/RFs) were the
same as those in TIMESVDPP_QUAL_7.749.


KNN_ON_TIMESVDPP_QUAL_7.243: This was generated on May 14 at 12:39 pm. kNN
with MIN_COMMON = 24 and MAX_WEIGHT = 200 was carried out on the residuals
of a Time-SVD++ run with 60 factors, 40 iterations, and 30 time bins (the
remaining parameters for the Time-SVD++ were the same as those in
TIMESVDPP_QUAL_7.749).


TIMESVDPP_QUAL_7.895: This was generated on May 14 at 1:25 am. Time-SVD++
was carried out on the whole dataset, with 345 factors and 40 iterations,
with all other parameters matching those in TIMESVDPP_QUAL_7.749 (including
the learning rate decay).


SVDPP_QUAL_7.001: This was generated on May 13 at 5:41 pm. SVD++ was
carried out on the whole dataset, with 1000 factors, 80 iterations, and a
learning rate decay of 0.935 (all other parameters remaining unchanged
relative to SVDPP_QUAL_6.833.


SVDPP_QUAL_6.703: This was generated on May 13 at 1:40 am. SVD++ was
carried out on the whole dataset, with 100 factors, 80 iterations, and a
learning rate decay of 0.93 (all other parameters remaining unchanged
relative to SVDPP_QUAL_6.833). This should've allowed for a small amount of
overfitting.


TIMESVDPP_QUAL_7.585: This was generated on May 12 at 10:03 am. Time-SVD++
was carried out on the whole dataset, with 60 factors, 30 time bins, 80
iterations, and userFacMatTime added. The learning rate decay was changed
to 0.915 (as opposed to 0.89) for more overfitting. But other than that,
the parameters are the same as in TIMESVDPP_QUAL_7.749.


SVDPP_QUAL_6.771: This was generated on May 11 at 6:41 pm. SVD++ was
carried out on the whole dataset, with 200 factors and 40 iterations. The
internal parameters were the same as in SVDPP_QUAL_6.833.


SVDPP_QUAL_6.833: This was generated on May 11 at 2:33 pm. SVD++ was
carried out on the whole dataset, with 500 factors and 40 iterations. The
internal parameters were as follows:
    * Regularization constants:
        * SVDPP_LAM_B_I = 0.005
        * SVDPP_LAM_B_U = 0.005
        * SVDPP_LAM_Q_I = 0.015
        * SVDPP_LAM_P_U = 0.015
        * SVDPP_LAM_Y_J = 0.015
    * Learning rates:
        * SVDPP_GAMMA_B_I = 0.007
        * SVDPP_GAMMA_B_U = 0.007
        * SVDPP_GAMMA_Q_I = 0.007
        * SVDPP_GAMMA_P_U = 0.007
        * SVDPP_GAMMA_Y_J = 0.007
    * Learning rate decay: 0.9 for all LRs.

And the following initializations were used:
    * userFacMat and itemFacMat's entries were randomly initialized to 
      (rand() % 4500 + 500) * 0.000001235 * copysign(1.0, coinFlip(engine))
      where the "copysign" part just generates a random sign.
    * bUser, bItem, and yMat were initialized to zero.


TIMESVDPP_QUAL_7.547: This was generated on May 11 at 4:50 am. Time-SVD++
was carried out on the whole dataset *without userFacMatTime*, with *100*
factors, 40 iterations, and 30 time bins. The remaining parameters
(LRs/RFs) were the same as those in TIMESVDPP_QUAL_7.749.


TIMESVDPP_QUAL_7.620: This was generated on May 10 at 1:14 am. Time-SVD++
was carried out on the whole dataset *without userFacMatTime*, with *200*
factors, 40 iterations, and 30 time bins. The remaining parameters (i.e.
LRs/RFs) were the same as those in TIMESVDPP_QUAL_7.749.


TIMESVDPP_QUAL_6.598: This was generated on May 9 at 8:32 pm. Time-SVD++
was carried out on the whole dataset *without userFacMatTime*, with *20*
factors, 40 iterations, and 30 time bins. The remaining parameters were the
same as those in TIMESVDPP_QUAL_7.749.


TIMESVDPP_QUAL_7.679: This was generated on May 9 at 6:01 pm. Time-SVD++
was carried out on the whole dataset *without userFacMatTime*. This run
used 500 factors, 40 iterations, and 30 time bins. The remaining parameters
(i.e. LRs/RFs) were the same as those in TIMESVDPP_QUAL_7.749.


TIMESVDPP_QUAL_7.598: This was generated on May 8 at 11:56 pm. Time-SVD++
was carried out on the entire training dataset with the same LRs/RFs as in
TIMESVDPP_QUAL_7.749, except *60 factors*, 40 iterations, and 30 time bins
were used.


TIMESVDPP_QUAL_6.803: This was generated on May 9 at 12:21 am. Time-SVD++
was carried out with the same parameters as in TIMESVDPP_QUAL_7.749, except
*20 factors*, 40 iterations, and 30 time bins were used. Note that this run
also used the entire dataset.


TIMESVDPP_QUAL_7.697: This was generated on May 8 at 8:31 am. Time-SVD++
was carried out with a learning rate decay of 0.9, 110 factors, 30 time
bins, and 80 iterations. The remaining parameters were the same as those
mentioned for TIMESVDPP_QUAL_7.749.dta. Note that this run also used the
entire training dataset.


GLOBALS_KNN_COMBO_QUAL_1.075: This was generated on May 8 at 3:19 am. kNN
was carried out on the residuals of 10 global effects, where both of these
algorithms were trained on the entire training dataset. For kNN, the
following parameters were used:
    * MIN_COMMON = 24
    * MAX_WEIGHT = 30


TIMESVDPP_QUAL_7.765: This was generated on May 7 at 10:00 am. The internal
parameters were unchanged relative to TIMESVDPP_QUAL_7.749, but I instead
carried out *40 iterations* of training with 110 factors and 30 time
bins. More than 40 iterations seems to lead to some amount of overfitting
(although it's very slight). Note that this run also used the entire
dataset.


TIMESVDPP_QUAL_7.749: This was generated on May 7 at 12:14 am. TimeSVD++
with frequency-dependent terms and item-dependent bin terms was carried out
for 25 iterations with 110 factors and 30 time bins (for item biases and
item factors). Training was carried out on all of the training data, and
testing was done on the "qual" set (the reported RMSE is a quiz RMSE). To
initialize the internal data of the TimeSVDPP, we used the following
approach:
    * Initialize userFacMat to random values in [-0.005, 0.005]
    * Initialize itemFacMat with a similar range of random values.
    * Initialize itemFacMatTimewise with a similar range of random values.
    * Initialize yMat with a similar range of random values.
    * Initialize cUserConst with 1s.
    * Initialize bUserConst, bUserAlpha, userFacMatAlpha, bItemConst,
      bItemTimewise, bItemFreq, and itemFacMatFreq to all zeros.
    * Initialize the sparse bUserTime and cUserTime matrices with small
      values of 1.0e-9.
    * Initialize userFacMatTime's vectors to all zeros.

As for our regularization constants and learning rates, we chose:
    * Regularization:
        * TIMESVDPP_LAM_B_U = 0.0065
        * TIMESVDPP_LAM_ALPHA_B_U = 0.0004
        * TIMESVDPP_LAM_B_U_T = 0.0050
        * TIMESVDPP_LAM_B_I = 0.005
        * TIMESVDPP_LAM_B_I_T = 0.0050
        * TIMESVDPP_LAM_B_I_F_U_T = 4.40e-3
        * TIMESVDPP_LAM_C_U = 0.010
        * TIMESVDPP_LAM_C_U_T = 0.0070
        * TIMESVDPP_LAM_Q_I = 0.0155
        * TIMESVDPP_LAM_Q_I_BIN = 0.022
        * TIMESVDPP_LAM_Q_I_F = 0.018
        * TIMESVDPP_LAM_P_U = 0.0155
        * TIMESVDPP_LAM_ALPHA_P_U = 0.0004
        * TIMESVDPP_LAM_P_U_T = 0.015
        * TIMESVDPP_LAM_Y_J = 0.0155
    * Learning rates:
        * TIMESVDPP_GAMMA_B_U = 0.0054
        * TIMESVDPP_GAMMA_ALPHA_B_U = 0.00003
        * TIMESVDPP_GAMMA_B_U_T = 0.0028
        * TIMESVDPP_GAMMA_B_I = 0.005
        * TIMESVDPP_GAMMA_B_I_T = 0.0001
        * TIMESVDPP_GAMMA_B_I_F_U_T = 0.00236
        * TIMESVDPP_GAMMA_C_U = 0.006
        * TIMESVDPP_GAMMA_C_U_T = 0.001
        * TIMESVDPP_GAMMA_Q_I = 0.005
        * TIMESVDPP_GAMMA_Q_I_BIN = 0.0007
        * TIMESVDPP_GAMMA_Q_I_F = 0.00003
        * TIMESVDPP_GAMMA_P_U = 0.0050
        * TIMESVDPP_GAMMA_ALPHA_P_U = 0.00001
        * TIMESVDPP_GAMMA_P_U_T = 0.0040
        * TIMESVDPP_GAMMA_Y_J = 0.0050
    * Learning rate decay (common to all rates):
        * TIMESVDPP_GAMMA_MULT_PER_ITER = 0.89


SVDPP_QUAL_6.239: This was generated on April 30 at 6:28 pm. SVD++ was run
for 25 iterations with 200 factors. Fewer iterations were chosen since 30
iterations were found to cause overfitting. Training was carried out on the
"base", "hidden", and "valid" sets, and testing was done on the "qual" set.

The regularization parameters, learning rates, and learning rate decays
were unchanged from before. All that was changed was initialization, as
follows:
    * bUser, bItem, and yMat were initialized to all zeros.
    * userFacMat and itemFacMat were initialized according to
      http://www.netflixprize.com/community/viewtopic.php?id=1342&p=3.


TIMESVDPP_QUAL_6.963: This was generated on April 30 at 3:48 pm. Unlike
previous versions, we used SVD++^(3) for this. We ran 130 factors for 30
iterations and with 30 time bins. Training was done on "base", "hidden",
and "valid", and testing was done on the "qual" set (the reported RMSE is
the "quiz" RMSE). To initialize the internal data of the TimeSVDPP, we used
the following approach:
    * Initialize bUserConst, bUserAlpha, userFacMatAlpha, bItemConst,
      bItemTimewise, and yMat to all zeros.
    * Initialize userFacMat and itemFacMat according to the suggestions in
      http://www.netflixprize.com/community/viewtopic.php?id=1342&p=3.
    * Batch-initialize bUserTime as before.
    * Fill the corresponding entries in userFacMatTime at the very
      beginning of train(). The vectors are filled with zeros.

As for our regularization parameters and learning rates, we used:
    * Regularization:
        * Same as before, with the addition of:
        * TIMESVDPP_LAM_P_U_T = 0.015;
    * Learning rates:
        * Same as before, with the addition of:
        * TIMESVDPP_GAMMA_P_U_T = 0.003
    * Learning rate decay (common to all rates): Same as before


TIMESVDPP_QUAL_6.904: This was generated on April 26 at 7:49 am. The
parameters are the same as the ones in TIMESVDPP_QUAL_6.829, except the
number of factors was increased to 500. Surprisingly, overfitting was
avoided (even though there are ~800 million parameters at this point, of
which ~770 million are user/movie features). Both the probe error and the
quiz error went down (by approximately the same RMSE of ~0.0007) relative
to the 200-factor case. This might be the best performance we can get out
of TimeSVD++, without adding even more sophistication (which would take up
too much memory).


TIMESVDPP_QUAL_6.829: This was generated on April 25 at 5:05 pm. TimeSVD++
was carried out for 30 iterations with 200 factors and 30 time bins (for
item biases). Training was carried out on the "base", "hidden", and "valid"
sets, and testing was done on the "qual" set (the reported RMSE is actually
the "quiz" RMSE of course). To initialize the internal data of the
TimeSVDPP, we used the following approach:
    * Initialize bUserConst, bUserAlpha, userFacMatAlpha, bItemConst,
      bItemTimewise, and yMat to all zeros. It makes sense for most of
      these to be zero since they're essentially item biases, user biases,
      and time biases.
    * The matrix bUserTime is a sparse matrix, but we batch-initialize its
      nonzero entries to 0.0000001 at the beginning of our training method
      (this cuts down on the first iteration's time; otherwise, we'd have
      to update the sparse matrix one by one).
    * The matrices userFacMat and itemFacMat have all of their entries
      initialized to std::rand() % 14000 + 2000) * 0.000001235 *
      std::copysign(1.0, coinFlip(engine), where the "copysign" part
      essentially just produces a random sign based on a "coin flip". This
      method was suggested in a Netflix forum post (see code).

As for our regularization constants and learning rates, we chose:
    * Regularization:
        * TIMESVDPP_LAM_B_U = 0.005
        * TIMESVDPP_LAM_ALPHA_B_U = 0.0004
        * TIMESVDPP_LAM_B_U_T = 0.005 
        * TIMESVDPP_LAM_B_I = 0.005
        * TIMESVDPP_LAM_B_I_T = 0.005
        * TIMESVDPP_LAM_Q_I = 0.015
        * TIMESVDPP_LAM_P_U = 0.015
        * TIMESVDPP_LAM_ALPHA_P_U = 0.0004
        * TIMESVDPP_LAM_Y_J = 0.015
    * Learning rates:
        * TIMESVDPP_GAMMA_B_U = 0.007;
        * TIMESVDPP_GAMMA_ALPHA_B_U = 0.00001;
        * TIMESVDPP_GAMMA_B_U_T = 0.007;
        * TIMESVDPP_GAMMA_B_I = 0.007;
        * TIMESVDPP_GAMMA_B_I_T = 0.007;
        * TIMESVDPP_GAMMA_Q_I = 0.007;
        * TIMESVDPP_GAMMA_P_U = 0.007;
        * TIMESVDPP_GAMMA_ALPHA_P_U = 0.00001;
        * TIMESVDPP_GAMMA_Y_J = 0.007;
    * Learning rate decay (common to all rates):
        * TIMESVDPP_GAMMA_MULT_PER_ITER = 0.9


SVDPP_QUAL_6.193: This was generated on April 17 at 3:17 am. SVD++ was run
for 30 iterations with 200 factors. Training was carried out on the "base",
"hidden", and "valid" sets, and testing was done on the "qual" set. The
following constants were used:
    * Regularization:
        * SVDPP_LAM_B_I = 0.005
        * SVDPP_LAM_B_U = 0.030
        * SVDPP_LAM_Q_I = 0.006
        * SVDPP_LAM_P_U = 0.080
        * SVDPP_LAM_Y_J = 0.030
    * Learning rates:
        * SVDPP_GAMMA_B_I = 0.003
        * SVDPP_GAMMA_B_U = 0.012
        * SVDPP_GAMMA_Q_I = 0.011
        * SVDPP_GAMMA_P_U = 0.006
        * SVDPP_GAMMA_Y_J = 0.001
    * Learning rate decay (common to all rates):
        * SVDPP_GAMMA_MULT_PER_ITER = 0.9


SVD with 200 features and 60 iterations: 0.91416 (3.91% above water)
    * alpha1, alpha2 are 0.008
    * beta1, beta2 are 0.01

