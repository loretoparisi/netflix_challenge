
===========================================================================
INTRODUCTION
===========================================================================

This is where we'll store some of our blended predictions, as well as the
quiz RMSEs they achieved (added on in the same format as in the
"quiz_blend" folder). Additional details on each blend are below.


===========================================================================
ADDITIONAL DETAILS
===========================================================================

BLEND_MAY_20_QRMSE_0.87034: Added a 30-factor overfitted Time-SVD++ run,
and ran the blending script with lambda = 0.0014. Coefficients are as
follows:
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40: 0.096
    * SVDPP_FAC_200_EPOCH_40: -0.140
    * SVDPP_FAC_500_EPOCH_40: -0.066
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40: 0.143
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40: -0.002
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40: 0.214
    * TIMESVDPP_FAC_20_EPOCH_40: 0.091
    * TIMESVDPP_FAC_110_EPOCH_80: -0.079
    * TIMESVDPP_FAC_110_EPOCH_40: -0.013
    * TIMESVDPP_FAC_60_EPOCH_80: -0.048
    * SVDPP_FAC_100_EPOCH_80: -0.154
    * SVDPP_FAC_1000_EPOCH_80: 0.011
    * TIMESVDPP_FAC_345_EPOCH_40: 0.134
    * TIMESVDPP_FAC_300_EPOCH_80: 0.245
    * SVDPP_FAC_2000_EPOCH_80: 0.271
    * SVD_FAC_1000_EPOCH_80: 0.022
    * RBM_FAC_200_EPOCH_60: 0.039
    * RBM_FAC_400_EPOCH_60: 0.029
    * GLOBALS_KNN_GE_10_MC_24_MW_30: 0.018
    * KNN_ON_TIMESVDPP_MC_24_MW_200_FAC_60_EPOCH_40: 0.051
    * KNN_ON_TIMESVDPP_MC_24_MW_400_FAC_60_EPOCH_40: 0.011
    * KNN_ON_TIMESVDPP_MC_30_MW_50_FAC_60_EPOCH_80: 0.041
    * SVD_FAC_2000_EPOCH_80: 0.050
    * TIMESVDPP_FAC_100_EPOCH_60: -0.131
    * TIMESVDPP_FAC_200_EPOCH_80: 0.111
    * TIMESVDPP_FAC_60_EPOCH_80: -0.159
    * KNN_ON_RBM_FACT_200_EPOCH_50_MC_30_MW_50: 0.018
    * KNN_ON_RBM_FAC_100_EPOCH_38_MC_30_MW_50: 0.032
    * KNN_ON_RBM_FAC_200_EPOCH_50_MC_16_MW_400: 0.054
    * KNN_ON_RBM_FAC_200_EPOCH_50_MC_30_MW_400: -0.003
    * KNN_ON_SGD_FACT_20_EPOCH_90_MC_30_MW_50: -0.029
    * RBM_FAC_200_EPOCH_57: -0.019
    * SGD_FAC_20_EPOCH_30: -0.009
    * SGD_FAC_20_EPOCH_90: 0.003
    * RBM_FAC_150_EPOCH_120: 0.051
    * RBM_FAC_250_EPOCH_120: 0.044
    * RBM_FAC_300_EPOCH_80: 0.049
    * SGD_FAC_100_EPOCH_90: -0.003
    * TIMESVDPP_FAC_30_EPOCH_80: 0.049


BLEND_MAY_20_QRMSE_0.87038: Added RBM result with 100, 150, 250 factors
with higher epochs and dec of 0.002. This result is with blending constant
0.0014.
    * SVDPP_FAC_200_EPOCH_40: -0.139
    * SVDPP_FAC_500_EPOCH_40: -0.065
    * TIMESVDPP_FAC_110_EPOCH_40: -0.012
    * TIMESVDPP_FAC_110_EPOCH_80: -0.073
    * TIMESVDPP_FAC_20_EPOCH_40: 0.116
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40: 0.089
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40: 0.134
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40: -0.004
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40: 0.203
    * TIMESVDPP_FAC_60_EPOCH_80: -0.030
    * SVDPP_FAC_1000_EPOCH_80: 0.008
    * SVDPP_FAC_100_EPOCH_80: -0.154
    * SVDPP_FAC_2000_EPOCH_80: 0.267
    * TIMESVDPP_FAC_300_EPOCH_80: 0.252
    * TIMESVDPP_FAC_345_EPOCH_40: 0.125
    * KNN_ON_RBM_FAC_200_EPOCH_50_MC_30_MW_400: -0.003
    * KNN_ON_RBM_FAC_200_EPOCH_50_MC_16_MW_400: 0.054
    * RBM_FAC_200_EPOCH_60: 0.039
    * RBM_FAC_400_EPOCH_60: 0.029
    * SVD_FAC_1000_EPOCH_80: 0.025
    * KNN_ON_RBM_FACT_200_EPOCH_50_MC_30_MW_50: 0.018
    * SGD_FAC_20_EPOCH_30: -0.009
    * RBM_FAC_200_EPOCH_57: -0.019
    * GLOBALS_KNN_GE_10_MC_24_MW_30: 0.017
    * KNN_ON_TIMESVDPP_MC_24_MW_200_FAC_60_EPOCH_40: 0.054
    * KNN_ON_TIMESVDPP_MC_24_MW_400_FAC_60_EPOCH_40: 0.015
    * KNN_ON_TIMESVDPP_MC_30_MW_50_FAC_60_EPOCH_80: 0.041
    * SVD_FAC_2000_EPOCH_80: 0.050
    * TIMESVDPP_FAC_100_EPOCH_60: -0.118
    * TIMESVDPP_FAC_200_EPOCH_80: 0.120
    * TIMESVDPP_FAC_60_EPOCH_80: -0.154
    * SGD_FAC_20_EPOCH_90: 0.003
    * KNN_ON_RBM_FAC_100_EPOCH_38_MC_30_MW_50: 0.032
    * KNN_ON_SGD_FACT_20_EPOCH_90_MC_30_MW_50: -0.030
    * SGD_FAC_100_EPOCH_90: -0.003
    * RBM_FAC_300_EPOCH_80: 0.049
    * RBM_FAC_150_EPOCH_120: 0.052
    * RBM_FAC_250_EPOCH_120: 0.044


BLEND_MAY_17_QRMSE_0.87052: Added RBM result with 100, 200, and 400 factors
and kNN on RBM for those factors. Also added SGD (GraphChi implementation
and kNN on SGD). This RMSE is with blend constant 0.001. With 0.0014, we are
at 0.87056, also 8.5%, rounded.
    * SVDPP_FAC_200_EPOCH_40: -0.147
    * SVDPP_FAC_500_EPOCH_40: -0.059
    * TIMESVDPP_FAC_110_EPOCH_40: -0.017
    * TIMESVDPP_FAC_110_EPOCH_80: -0.092
    * TIMESVDPP_FAC_20_EPOCH_40: 0.124
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40: 0.095
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40: 0.139
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40: -0.009
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40: 0.218
    * TIMESVDPP_FAC_60_EPOCH_80: -0.017
    * SVDPP_FAC_1000_EPOCH_80: -0.025
    * SVDPP_FAC_100_EPOCH_80: -0.147
    * SVDPP_FAC_2000_EPOCH_80: 0.302
    * TIMESVDPP_FAC_300_EPOCH_80: 0.266
    * TIMESVDPP_FAC_345_EPOCH_40: 0.138
    * KNN_ON_RBM_FAC_200_EPOCH_50_MC_30_MW_400: 0.004
    * KNN_ON_RBM_FAC_200_EPOCH_50_MC_16_MW_400: 0.055
    * RBM_FAC_200_EPOCH_60: 0.087
    * RBM_FAC_400_EPOCH_60: 0.063
    * SVD_FAC_1000_EPOCH_80: 0.026
    * KNN_ON_RBM_FACT_200_EPOCH_50_MC_30_MW_50: 0.020
    * SGD_FAC_20_EPOCH_30: -0.010
    * RBM_FAC_200_EPOCH_57: -0.001
    * GLOBALS_KNN_GE_10_MC_24_MW_30: 0.017
    * KNN_ON_TIMESVDPP_MC_24_MW_200_FAC_60_EPOCH_40: 0.056
    * KNN_ON_TIMESVDPP_MC_24_MW_400_FAC_60_EPOCH_40: 0.009
    * KNN_ON_TIMESVDPP_MC_30_MW_50_FAC_60_EPOCH_80: 0.037
    * SVD_FAC_2000_EPOCH_80: 0.062
    * TIMESVDPP_FAC_100_EPOCH_60: -0.137
    * TIMESVDPP_FAC_200_EPOCH_80: 0.121
    * TIMESVDPP_FAC_60_EPOCH_80: -0.161
    * SGD_FAC_20_EPOCH_90: 0.002
    * KNN_ON_RBM_FAC_100_EPOCH_38_MC_30_MW_50: 0.038
    * KNN_ON_SGD_FACT_20_EPOCH_90_MC_30_MW_50: -0.031


BLEND_MAY_17_QRMSE_0.87093: Added a lot of predictors back to the
quiz_blend folder, and ran quiz-blending again. This gave the following
coefficients:
    * SVDPP_FAC_200_EPOCH_40: -0.137
    * SVDPP_FAC_500_EPOCH_40: -0.064
    * TIMESVDPP_FAC_110_EPOCH_40: -0.011
    * TIMESVDPP_FAC_110_EPOCH_80: -0.073
    * TIMESVDPP_FAC_20_EPOCH_40: 0.132
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40: 0.084
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40: 0.129
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40: -0.011
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40: 0.198
    * GLOBALS_KNN_GE_10_MC_24_MW_30: 0.010
    * TIMESVDPP_FAC_60_EPOCH_80: -0.027
    * SVDPP_FAC_100_EPOCH_80: -0.147
    * SVDPP_FAC_1000_EPOCH_80: 0.016
    * TIMESVDPP_FAC_345_EPOCH_40: 0.127
    * TIMESVDPP_FAC_300_EPOCH_80: 0.250
    * SVDPP_FAC_2000_EPOCH_80: 0.280
    * KNN_ON_TIMESVDPP_MC_24_MW_200_FAC_60_EPOCH_40: 0.062
    * KNN_ON_TIMESVDPP_MC_24_MW_400_FAC_60_EPOCH_40: 0.024
    * SVD_FAC_1000_EPOCH_80: 0.037
    * RBM_FAC_200_EPOCH_60: 0.116
    * RBM_FAC_400_EPOCH_60: 0.084
    * SVD_FAC_2000_EPOCH_80: 0.060
    * TIMESVDPP_FAC_200_EPOCH_80: 0.117
    * TIMESVDPP_FAC_100_EPOCH_60: -0.122
    * KNN_ON_TIMESVDPP_MC_30_MW_50_FAC_60_EPOCH_80: 0.071
    * TIMESVDPP_FAC_60_EPOCH_80: -0.176


BLEND_MAY_17_QRMSE_0.87131: After fixing some bugs in the quiz blending
script, I ran a blend on the same predictors as below. This gave the
following coefficients:
    * SVDPP_FAC_200_EPOCH_40: -0.129
    * SVDPP_FAC_500_EPOCH_40: -0.042
    * TIMESVDPP_FAC_110_EPOCH_40: -0.013
    * TIMESVDPP_FAC_110_EPOCH_80: -0.103
    * TIMESVDPP_FAC_20_EPOCH_40: 0.149
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40: 0.047
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40: 0.105
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40: -0.002
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40: 0.187
    * TIMESVDPP_FAC_60_EPOCH_80: -0.046
    * SVDPP_FAC_100_EPOCH_80: -0.160
    * SVDPP_FAC_1000_EPOCH_80: 0.000
    * TIMESVDPP_FAC_345_EPOCH_40: 0.159
    * TIMESVDPP_FAC_300_EPOCH_80: 0.278
    * SVDPP_FAC_2000_EPOCH_80: 0.292
    * SVD_FAC_1000_EPOCH_80: 0.091
    * RBM_FAC_200_EPOCH_60: 0.123
    * RBM_FAC_400_EPOCH_60: 0.088


BLEND_MAY_15_QRMSE_0.87278: This came from a quiz blend of the following
predictors, all of which were trained on the entire dataset. Blending
coefficients have been included as well.
    * SVDPP_FAC_200_EPOCH_40: -0.113
    * SVDPP_FAC_500_EPOCH_40: -0.124
    * TIMESVDPP_FAC_110_EPOCH_40: -0.107
    * TIMESVDPP_FAC_110_EPOCH_80: -0.044
    * TIMESVDPP_FAC_20_EPOCH_40: 0.259
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40: 0.077
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40: 0.087
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40: -0.014
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40: 0.081
    * TIMESVDPP_FAC_60_EPOCH_80: -0.018
    * SVDPP_FAC_100_EPOCH_80: -0.090
    * SVDPP_FAC_1000_EPOCH_80: 0.102
    * TIMESVDPP_FAC_345_EPOCH_40: -0.214
    * TIMESVDPP_FAC_300_EPOCH_80: 0.541
    * SVDPP_FAC_2000_EPOCH_80: 0.174
    * SVD_FAC_1000_EPOCH_80: 0.057
    * RBM_FAC_200_EPOCH_60: 0.165
    * RBM_FAC_400_EPOCH_60: 0.180

The overfitted SVD and the RBMs seem to be improving the blend noticeably.
I also removed the old 200-factor RBM that was trained on just base.


BLEND_MAY_14_QRMSE_0.87358: This came from a quiz blend of the following
predictors. All but the RBM were trained on the entire dataset:
    * RBM_FAC_200_EPOCH_36_QRMSE_0.91109.dta
    * SVDPP_FAC_1000_EPOCH_80_QRMSE_0.88479.dta
    * SVDPP_FAC_100_EPOCH_80_QRMSE_0.88763.dta
    * SVDPP_FAC_2000_EPOCH_80_QRMSE_0.88415.dta
    * SVDPP_FAC_200_EPOCH_40_QRMSE_0.88698.dta
    * SVDPP_FAC_500_EPOCH_40_QRMSE_0.88639.dta
    * TIMESVDPP_FAC_110_EPOCH_40_QRMSE_0.87752.dta
    * TIMESVDPP_FAC_110_EPOCH_80_QRMSE_0.87817.dta
    * TIMESVDPP_FAC_20_EPOCH_40_QRMSE_0.88668.dta
    * TIMESVDPP_FAC_300_EPOCH_80_QRMSE_0.88275.dta
    * TIMESVDPP_FAC_345_EPOCH_40_QRMSE_0.87629.dta
    * TIMESVDPP_FAC_60_EPOCH_80_QRMSE_0.87924.dta
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40_QRMSE_0.8796.dta
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40_QRMSE_0.8789.dta
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40_QRMSE_0.88863.dta
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40_QRMSE_0.87834.dta

The RBM improved prediction performance, but could use additional
tweaking.


BLEND_MAY_14_QRMSE_0.87374: This came from a quiz blend of the following
predictors, all of which were trained on the entire dataset:
    * KNN_ON_TIMESVDPP_MC_24_MW_200_FAC_60_EPOCH_40_QRMSE_0.88249.dta
    * SVDPP_FAC_1000_EPOCH_80_QRMSE_0.88479.dta
    * SVDPP_FAC_100_EPOCH_80_QRMSE_0.88763.dta
    * SVDPP_FAC_2000_EPOCH_80_QRMSE_0.88415.dta
    * SVDPP_FAC_200_EPOCH_40_QRMSE_0.88698.dta
    * SVDPP_FAC_500_EPOCH_40_QRMSE_0.88639.dta
    * TIMESVDPP_FAC_110_EPOCH_40_QRMSE_0.87752.dta
    * TIMESVDPP_FAC_110_EPOCH_80_QRMSE_0.87817.dta
    * TIMESVDPP_FAC_20_EPOCH_40_QRMSE_0.88668.dta
    * TIMESVDPP_FAC_300_EPOCH_80_QRMSE_0.88275.dta
    * TIMESVDPP_FAC_345_EPOCH_40_QRMSE_0.87629.dta
    * TIMESVDPP_FAC_60_EPOCH_80_QRMSE_0.87924.dta
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40_QRMSE_0.8796.dta
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40_QRMSE_0.8789.dta
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40_QRMSE_0.88863.dta
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40_QRMSE_0.87834.dta

Note that the new, 2000-factor SVD++ is contributing a noticeable
improvement. The old 1000-factor SVD++ is not contributing as much now.


BLEND_MAY_14_QRMSE_0.87393: This came from a quiz blend of the following
predictors, all of which were trained on the entire dataset:
    * KNN_ON_TIMESVDPP_MC_24_MW_200_FAC_60_EPOCH_40_QRMSE_0.88249.dta
    * SVDPP_FAC_1000_EPOCH_80_QRMSE_0.88479.dta
    * SVDPP_FAC_100_EPOCH_80_QRMSE_0.88763.dta
    * SVDPP_FAC_200_EPOCH_40_QRMSE_0.88698.dta
    * SVDPP_FAC_500_EPOCH_40_QRMSE_0.88639.dta
    * TIMESVDPP_FAC_110_EPOCH_40_QRMSE_0.87752.dta
    * TIMESVDPP_FAC_110_EPOCH_80_QRMSE_0.87817.dta
    * TIMESVDPP_FAC_20_EPOCH_40_QRMSE_0.88668.dta
    * TIMESVDPP_FAC_300_EPOCH_80_QRMSE_0.88275.dta
    * TIMESVDPP_FAC_345_EPOCH_40_QRMSE_0.87629.dta
    * TIMESVDPP_FAC_60_EPOCH_80_QRMSE_0.87924.dta
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40_QRMSE_0.8796.dta
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40_QRMSE_0.8789.dta
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40_QRMSE_0.88863.dta
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40_QRMSE_0.87834.dta 

The new 300-factor, overfitted Time-SVD++ significantly changed the
characteristics of the blend, which was interesting since it's QRMSE was
fairly low.


BLEND_MAY_13_QRMSE_0.87418: This came from a quiz blend of the following
predictors, all of which were trained on the entire dataset:
    * SVDPP_FAC_1000_EPOCH_80_QRMSE_0.88479.dta
    * SVDPP_FAC_100_EPOCH_80_QRMSE_0.88763.dta
    * SVDPP_FAC_200_EPOCH_40_QRMSE_0.88698.dta
    * SVDPP_FAC_500_EPOCH_40_QRMSE_0.88639.dta
    * TIMESVDPP_FAC_110_EPOCH_40_QRMSE_0.87752.dta
    * TIMESVDPP_FAC_110_EPOCH_80_QRMSE_0.87817.dta
    * TIMESVDPP_FAC_20_EPOCH_40_QRMSE_0.88668.dta
    * TIMESVDPP_FAC_60_EPOCH_80_QRMSE_0.87924.dta
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40_QRMSE_0.8796.dta
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40_QRMSE_0.8789.dta
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40_QRMSE_0.88863.dta
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40_QRMSE_0.87834.dta

The biggest contributor to this blend was the 1000-factor overfitted SVD++
prediction; it had a weight of around 59%. The next biggest contributor was
TIMESVDPP_FAC_20_EPOCH_40, interestingly enough.


BLEND_MAY_11_QRMSE_0.87552: This came from a quiz blend of the following
qual prediction data, all of which were trained on the entire dataset:
    * SVDPP_FAC_200_EPOCH_40_QRMSE_0.88698.dta
    * SVDPP_FAC_500_EPOCH_40_QRMSE_0.88639.dta
    * TIMESVDPP_FAC_110_EPOCH_40_QRMSE_0.87752.dta
    * TIMESVDPP_FAC_110_EPOCH_80_QRMSE_0.87817.dta
    * TIMESVDPP_FAC_20_EPOCH_40_QRMSE_0.88668.dta
    * TIMESVDPP_FAC_60_EPOCH_40_QRMSE_0.87911.dta
    * TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40_QRMSE_0.8796.dta
    * TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40_QRMSE_0.8789.dta
    * TIMESVDPP_NO_UFMT_FAC_20_EPOCH_40_QRMSE_0.88863.dta
    * TIMESVDPP_NO_UFMT_FAC_500_EPOCH_40_QRMSE_0.87834.dta 

The improvement over the previous set of blended predictions is only
marginal. Possible reason: negative coefficients were produced for
TIMESVDPP_NO_UFMT_FAC_200_EPOCH_40, TIMESVDPP_NO_UFMT_FAC_100_EPOCH_40
(huge negative coefficient), and TIMESVDPP_FAC_60_EPOCH_40. Perhaps these
didn't work as well with the other data...


BLEND_MAY_07_QRMSE_0.87651: This came from a quiz blend that added on to the
previous blend. The following qual prediction data was used:
    * SVDPP_FAC_200_EPOCH_25_QRMSE_0.89204.dta
    * TIMESVDPP_FAC_110_EPOCH_25_QRMSE_0.87768.dta (new)
    * TIMESVDPP_FAC_130_EPOCH_30_QRMSE_0.88515.dta
    * TIMESVDPP_FAC_500_EPOCH_30_QRMSE_0.88572.dta


Note that only TIMESVDPP_FAC_110_EPOCH_25_QRMSE_0.87768.dta was trained on
the entire dataset, while the others were trained on just base, hidden, and
valid. Also, TIMESVDPP_FAC_130_EPOCH_30_QRMSE_0.88515.dta actually had a
negative weight in this blend (indicating possible issues with blending too
many Time-SVD++ models with similar factors?)


BLEND_MAY_03_QRMSE_0.88427: This came from a quiz blend (that I (Laksh)
tried as a test) on the following qual prediction data:
    * SVDPP_FAC_200_EPOCH_25_QRMSE_0.89204.dta
    * TIMESVDPP_FAC_130_EPOCH_30_QRMSE_0.88515.dta
    * TIMESVDPP_FAC_500_EPOCH_30_QRMSE_0.88572.dta

Note that none of these data files were trained on all of the dataset, so
the quiz blend didn't give as good of a result as one would expect.


